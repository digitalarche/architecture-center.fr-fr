---
title: Décisionnel d’entreprise automatisé
titleSuffix: Azure Reference Architectures
description: Automatisez un workflow ELT (Extract-Load-Transform) dans Azure à l’aide d’Azure Data Factory avec SQL Data Warehouse.
author: MikeWasson
ms.date: 11/06/2018
ms.custom: seodec18
ms.openlocfilehash: 579ef0361ec44d0eb82b9076490eed5a6d88df35
ms.sourcegitcommit: cd3de23543f739a95a1daf38886561f67add9d64
ms.translationtype: HT
ms.contentlocale: fr-FR
ms.lasthandoff: 01/10/2019
ms.locfileid: "54183590"
---
# <a name="automated-enterprise-bi-with-sql-data-warehouse-and-azure-data-factory"></a><span data-ttu-id="5a743-103">BI d’entreprise automatisée avec SQL Data Warehouse et Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="5a743-103">Automated enterprise BI with SQL Data Warehouse and Azure Data Factory</span></span>

<span data-ttu-id="5a743-104">Cette architecture de référence indique comment effectuer un chargement incrémentiel dans un pipeline [ELT (Extract-Load-Transform)](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt).</span><span class="sxs-lookup"><span data-stu-id="5a743-104">This reference architecture shows how to perform incremental loading in an [extract, load, and transform (ELT)](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt) pipeline.</span></span> <span data-ttu-id="5a743-105">Elle utilise Azure Data Factory pour automatiser le pipeline ELT.</span><span class="sxs-lookup"><span data-stu-id="5a743-105">It uses Azure Data Factory to automate the ELT pipeline.</span></span> <span data-ttu-id="5a743-106">Ce pipeline déplace de façon incrémentielle les données OLTP les plus récentes d’une base de données SQL Server locale vers une instance SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="5a743-106">The pipeline incrementally moves the latest OLTP data from an on-premises SQL Server database into SQL Data Warehouse.</span></span> <span data-ttu-id="5a743-107">Les données transactionnelles sont transformées en un modèle tabulaire à des fins d’analyse.</span><span class="sxs-lookup"><span data-stu-id="5a743-107">Transactional data is transformed into a tabular model for analysis.</span></span>

> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE2Gnz2]

<span data-ttu-id="5a743-108">Une implémentation de référence pour cette architecture est disponible sur [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="5a743-108">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Diagramme d’architecture pour le décisionnel d’entreprise automatisé avec SQL Data Warehouse et Azure Data Factory](./images/enterprise-bi-sqldw-adf.png)

<span data-ttu-id="5a743-110">Cette architecture repose sur celle qui est décrite dans la section [Enterprise BI avec SQL Data Warehouse](./enterprise-bi-sqldw.md), mais ajoute des fonctionnalités qui sont importantes pour les scénarios d’entreposage de données d’entreprise.</span><span class="sxs-lookup"><span data-stu-id="5a743-110">This architecture builds on the one shown in [Enterprise BI with SQL Data Warehouse](./enterprise-bi-sqldw.md), but adds some features that are important for enterprise data warehousing scenarios.</span></span>

- <span data-ttu-id="5a743-111">Automatisation du pipeline à l’aide de Data Factory.</span><span class="sxs-lookup"><span data-stu-id="5a743-111">Automation of the pipeline using Data Factory.</span></span>
- <span data-ttu-id="5a743-112">Chargement incrémentiel.</span><span class="sxs-lookup"><span data-stu-id="5a743-112">Incremental loading.</span></span>
- <span data-ttu-id="5a743-113">Intégration de plusieurs sources de données.</span><span class="sxs-lookup"><span data-stu-id="5a743-113">Integrating multiple data sources.</span></span>
- <span data-ttu-id="5a743-114">Chargement des données binaires comme les données géospatiales et les images.</span><span class="sxs-lookup"><span data-stu-id="5a743-114">Loading binary data such as geospatial data and images.</span></span>

## <a name="architecture"></a><span data-ttu-id="5a743-115">Architecture</span><span class="sxs-lookup"><span data-stu-id="5a743-115">Architecture</span></span>

<span data-ttu-id="5a743-116">L’architecture est constituée des composants suivants.</span><span class="sxs-lookup"><span data-stu-id="5a743-116">The architecture consists of the following components.</span></span>

### <a name="data-sources"></a><span data-ttu-id="5a743-117">Sources de données</span><span class="sxs-lookup"><span data-stu-id="5a743-117">Data sources</span></span>

<span data-ttu-id="5a743-118">**Serveur SQL Server local**.</span><span class="sxs-lookup"><span data-stu-id="5a743-118">**On-premises SQL Server**.</span></span> <span data-ttu-id="5a743-119">Les données sources sont situées dans une base de données SQL Server locale.</span><span class="sxs-lookup"><span data-stu-id="5a743-119">The source data is located in a SQL Server database on premises.</span></span> <span data-ttu-id="5a743-120">Pour simuler l’environnement local, les scripts de déploiement de cette architecture approvisionnent une machine virtuelle dans Azure disposant de SQL Server.</span><span class="sxs-lookup"><span data-stu-id="5a743-120">To simulate the on-premises environment, the deployment scripts for this architecture provision a virtual machine in Azure with SQL Server installed.</span></span> <span data-ttu-id="5a743-121">[L’exemple de base de données OLTP Wide World Importers][wwi] est utilisé comme base de données source.</span><span class="sxs-lookup"><span data-stu-id="5a743-121">The [Wide World Importers OLTP sample database][wwi] is used as the source database.</span></span>

<span data-ttu-id="5a743-122">**Données externes**.</span><span class="sxs-lookup"><span data-stu-id="5a743-122">**External data**.</span></span> <span data-ttu-id="5a743-123">Un scénario courant de gestion des entrepôts de données consiste à intégrer plusieurs sources de données.</span><span class="sxs-lookup"><span data-stu-id="5a743-123">A common scenario for data warehouses is to integrate multiple data sources.</span></span> <span data-ttu-id="5a743-124">Cette architecture de référence charge un jeu de données externe qui contient la population par ville et par année, et l’intègre dans les données de la base de données OLTP.</span><span class="sxs-lookup"><span data-stu-id="5a743-124">This reference architecture loads an external data set that contains city populations by year, and integrates it with the data from the OLTP database.</span></span> <span data-ttu-id="5a743-125">Vous pouvez utiliser ces données pour obtenir des insights du type : « La croissance des ventes dans chaque région correspond-elle, voire dépasse-t-elle la croissance de la population ? »</span><span class="sxs-lookup"><span data-stu-id="5a743-125">You can use this data for insights such as: "Does sales growth in each region match or exceed population growth?"</span></span>

### <a name="ingestion-and-data-storage"></a><span data-ttu-id="5a743-126">Ingestion et stockage de données</span><span class="sxs-lookup"><span data-stu-id="5a743-126">Ingestion and data storage</span></span>

<span data-ttu-id="5a743-127">**Stockage d'objets blob**.</span><span class="sxs-lookup"><span data-stu-id="5a743-127">**Blob Storage**.</span></span> <span data-ttu-id="5a743-128">Le stockage d’objets blob est utilisé en tant que zone de processus de site pour les données sources, avant leur chargement dans SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="5a743-128">Blob storage is used as a staging area for the source data before loading it into SQL Data Warehouse.</span></span>

<span data-ttu-id="5a743-129">**Azure SQL Data Warehouse**.</span><span class="sxs-lookup"><span data-stu-id="5a743-129">**Azure SQL Data Warehouse**.</span></span> <span data-ttu-id="5a743-130">[SQL Data Warehouse](/azure/sql-data-warehouse/) est un système distribué conçu pour réaliser des analyses sur de grandes quantités de données.</span><span class="sxs-lookup"><span data-stu-id="5a743-130">[SQL Data Warehouse](/azure/sql-data-warehouse/) is a distributed system designed to perform analytics on large data.</span></span> <span data-ttu-id="5a743-131">Il prend en charge le traitement MPP (Massive Parallel Processing), le rendant ainsi adapté à l’exécution d’analyses hautes performances.</span><span class="sxs-lookup"><span data-stu-id="5a743-131">It supports massive parallel processing (MPP), which makes it suitable for running high-performance analytics.</span></span>

<span data-ttu-id="5a743-132">**Azure Data Factory**.</span><span class="sxs-lookup"><span data-stu-id="5a743-132">**Azure Data Factory**.</span></span> <span data-ttu-id="5a743-133">[Data Factory][adf] est un service géré qui orchestre et automatise le déplacement et la transformation des données.</span><span class="sxs-lookup"><span data-stu-id="5a743-133">[Data Factory][adf] is a managed service that orchestrates and automates data movement and data transformation.</span></span> <span data-ttu-id="5a743-134">Dans cette architecture, il coordonne les différentes étapes du processus ELT.</span><span class="sxs-lookup"><span data-stu-id="5a743-134">In this architecture, it coordinates the various stages of the ELT process.</span></span>

### <a name="analysis-and-reporting"></a><span data-ttu-id="5a743-135">Analyse et rapports</span><span class="sxs-lookup"><span data-stu-id="5a743-135">Analysis and reporting</span></span>

<span data-ttu-id="5a743-136">**Azure Analysis Services**.</span><span class="sxs-lookup"><span data-stu-id="5a743-136">**Azure Analysis Services**.</span></span> <span data-ttu-id="5a743-137">[Analysis Services](/azure/analysis-services/) est un service entièrement géré qui fournit des capacités de modélisation des données.</span><span class="sxs-lookup"><span data-stu-id="5a743-137">[Analysis Services](/azure/analysis-services/) is a fully managed service that provides data modeling capabilities.</span></span> <span data-ttu-id="5a743-138">Le modèle sémantique est chargé dans Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="5a743-138">The semantic model is loaded into Analysis Services.</span></span>

<span data-ttu-id="5a743-139">**Power BI**.</span><span class="sxs-lookup"><span data-stu-id="5a743-139">**Power BI**.</span></span> <span data-ttu-id="5a743-140">Power BI est une suite d’outils d’analyse métier pour analyser les données et obtenir des informations métier.</span><span class="sxs-lookup"><span data-stu-id="5a743-140">Power BI is a suite of business analytics tools to analyze data for business insights.</span></span> <span data-ttu-id="5a743-141">Dans cette architecture, il demande le modèle sémantique stocké dans Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="5a743-141">In this architecture, it queries the semantic model stored in Analysis Services.</span></span>

### <a name="authentication"></a><span data-ttu-id="5a743-142">Authentification</span><span class="sxs-lookup"><span data-stu-id="5a743-142">Authentication</span></span>

<span data-ttu-id="5a743-143">**Azure Active Directory** (Azure AD) authentifie les utilisateurs qui se connectent au serveur Analysis Services via Power BI.</span><span class="sxs-lookup"><span data-stu-id="5a743-143">**Azure Active Directory** (Azure AD) authenticates users who connect to the Analysis Services server through Power BI.</span></span>

<span data-ttu-id="5a743-144">Data Factory peut également utiliser Azure AD pour s’authentifier auprès de SQL Data Warehouse, en utilisant un principal de service ou une identité MSI (Managed Service Identity).</span><span class="sxs-lookup"><span data-stu-id="5a743-144">Data Factory can use also use Azure AD to authenticate to SQL Data Warehouse, by using a service principal or Managed Service Identity (MSI).</span></span> <span data-ttu-id="5a743-145">Par souci de simplicité, l’exemple de déploiement utilise l’authentification SQL Server.</span><span class="sxs-lookup"><span data-stu-id="5a743-145">For simplicity, the example deployment uses SQL Server authentication.</span></span>

## <a name="data-pipeline"></a><span data-ttu-id="5a743-146">Pipeline de données</span><span class="sxs-lookup"><span data-stu-id="5a743-146">Data pipeline</span></span>

<span data-ttu-id="5a743-147">Dans [Azure Data Factory][adf], un pipeline est un regroupement logique d’activités qui permet de coordonner une tâche &mdash;. Dans ce cas, il s’agit du chargement et de la transformation des données dans SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="5a743-147">In [Azure Data Factory][adf], a pipeline is a logical grouping of activities used to coordinate a task &mdash; in this case, loading and transforming data into SQL Data Warehouse.</span></span>

<span data-ttu-id="5a743-148">Cette architecture de référence définit un pipeline principal qui exécute une suite de pipelines enfants.</span><span class="sxs-lookup"><span data-stu-id="5a743-148">This reference architecture defines a master pipeline that runs a sequence of child pipelines.</span></span> <span data-ttu-id="5a743-149">Chaque pipeline enfant charge les données dans une ou plusieurs tables d’entrepôt de données.</span><span class="sxs-lookup"><span data-stu-id="5a743-149">Each child pipeline loads data into one or more data warehouse tables.</span></span>

![Capture d’écran du pipeline dans Azure Data Factory](./images/adf-pipeline.png)

## <a name="incremental-loading"></a><span data-ttu-id="5a743-151">Chargement incrémentiel</span><span class="sxs-lookup"><span data-stu-id="5a743-151">Incremental loading</span></span>

<span data-ttu-id="5a743-152">Lorsque vous exécutez un processus ETL ou ELT automatisé, il s’avère plus efficace de charger uniquement les données modifiées depuis l’exécution précédente.</span><span class="sxs-lookup"><span data-stu-id="5a743-152">When you run an automated ETL or ELT process, it's most efficient to load only the data that changed since the previous run.</span></span> <span data-ttu-id="5a743-153">Il s’agit d’un *chargement incrémentiel*, par opposition à un chargement complet, qui porte sur toutes les données.</span><span class="sxs-lookup"><span data-stu-id="5a743-153">This is called an *incremental load*, as opposed to a full load that loads all of the data.</span></span> <span data-ttu-id="5a743-154">Pour effectuer un chargement incrémentiel, vous avez besoin d’une méthode d’identification des données modifiées.</span><span class="sxs-lookup"><span data-stu-id="5a743-154">To perform an incremental load, you need a way to identify which data has changed.</span></span> <span data-ttu-id="5a743-155">L’approche la plus courante consiste à utiliser une valeur *limite supérieure*, qui se traduit par le suivi de la valeur la plus récente d’une colonne de la table source, soit une colonne DateHeure, soit une colonne d’entier unique.</span><span class="sxs-lookup"><span data-stu-id="5a743-155">The most common approach is to use a *high water mark* value, which means tracking the latest value of some column in the source table, either a datetime column or a unique integer column.</span></span>

<span data-ttu-id="5a743-156">Depuis SQL Server 2016, vous pouvez utiliser des [tables temporelles](/sql/relational-databases/tables/temporal-tables).</span><span class="sxs-lookup"><span data-stu-id="5a743-156">Starting with SQL Server 2016, you can use [temporal tables](/sql/relational-databases/tables/temporal-tables).</span></span> <span data-ttu-id="5a743-157">Il s’agit de tables de système par version qui conservent un historique complet des modifications apportées aux données.</span><span class="sxs-lookup"><span data-stu-id="5a743-157">These are system-versioned tables that keep a full history of data changes.</span></span> <span data-ttu-id="5a743-158">Le moteur de base de données enregistre automatiquement l’historique de chaque modification dans une table d’historique distincte.</span><span class="sxs-lookup"><span data-stu-id="5a743-158">The database engine automatically records the history of every change in a separate history table.</span></span> <span data-ttu-id="5a743-159">Vous pouvez interroger les données d’historique en ajoutant une clause FOR SYSTEM_TIME à une requête.</span><span class="sxs-lookup"><span data-stu-id="5a743-159">You can query the historical data by adding a FOR SYSTEM_TIME clause to a query.</span></span> <span data-ttu-id="5a743-160">En interne, le moteur de base de données interroge la table d’historique, mais cette opération est transparente pour l’application.</span><span class="sxs-lookup"><span data-stu-id="5a743-160">Internally, the database engine queries the history table, but this is transparent to the application.</span></span>

> [!NOTE]
> <span data-ttu-id="5a743-161">Pour les versions antérieures de SQL Server, vous pouvez utiliser la fonction [Capture des changements de données](/sql/relational-databases/track-changes/about-change-data-capture-sql-server) (CDC).</span><span class="sxs-lookup"><span data-stu-id="5a743-161">For earlier versions of SQL Server, you can use [Change Data Capture](/sql/relational-databases/track-changes/about-change-data-capture-sql-server) (CDC).</span></span> <span data-ttu-id="5a743-162">Cette approche est moins pratique que les tables temporelles, car vous devez interroger une table de modifications distincte, et les modifications font l’objet d’un suivi par numéro séquentiel dans le journal plutôt que par horodatage.</span><span class="sxs-lookup"><span data-stu-id="5a743-162">This approach is less convenient than temporal tables, because you have to query a separate change table, and changes are tracked by a log sequence number, rather than a timestamp.</span></span>
>

<span data-ttu-id="5a743-163">Les tables temporelles sont utiles pour les données de dimension, qui peuvent changer au fil du temps.</span><span class="sxs-lookup"><span data-stu-id="5a743-163">Temporal tables are useful for dimension data, which can change over time.</span></span> <span data-ttu-id="5a743-164">Les tables de faits représentent généralement une transaction immuable, comme une vente. De ce fait, il n’est pas utile de conserver l’historique des versions du système.</span><span class="sxs-lookup"><span data-stu-id="5a743-164">Fact tables usually represent an immutable transaction such as a sale, in which case keeping the system version history doesn't make sense.</span></span> <span data-ttu-id="5a743-165">Au lieu de cela, les transactions incluent généralement une colonne qui représente la date de transaction, qui peut être utilisée en tant que la valeur de filigrane.</span><span class="sxs-lookup"><span data-stu-id="5a743-165">Instead, transactions usually have a column that represents the transaction date, which can be used as the watermark value.</span></span> <span data-ttu-id="5a743-166">Par exemple, dans la base de données OLTP Wide World Importers, les tables Sales.Invoices et Sales.InvoiceLines incluent un champ `LastEditedWhen` dont la valeur par défaut est `sysdatetime()`.</span><span class="sxs-lookup"><span data-stu-id="5a743-166">For example, in the Wide World Importers OLTP database, the Sales.Invoices and Sales.InvoiceLines tables have a `LastEditedWhen` field that defaults to `sysdatetime()`.</span></span>

<span data-ttu-id="5a743-167">Voici le flux général du pipeline ELT :</span><span class="sxs-lookup"><span data-stu-id="5a743-167">Here is the general flow for the ELT pipeline:</span></span>

1. <span data-ttu-id="5a743-168">Pour chaque table de la base de données source, effectuez le suivi de l’heure de coupure lors de l’exécution du dernier travail ELT.</span><span class="sxs-lookup"><span data-stu-id="5a743-168">For each table in the source database, track the cutoff time when the last ELT job ran.</span></span> <span data-ttu-id="5a743-169">Stockez ces informations dans l’entrepôt de données.</span><span class="sxs-lookup"><span data-stu-id="5a743-169">Store this information in the data warehouse.</span></span> <span data-ttu-id="5a743-170">(Lors de la configuration initiale, toutes les heures sont définies sur « 1-1-1900 ».)</span><span class="sxs-lookup"><span data-stu-id="5a743-170">(On initial setup, all times are set to '1-1-1900'.)</span></span>

2. <span data-ttu-id="5a743-171">Lors de l’étape d’exportation des données, l’heure de coupure est transmise sous la forme d’un paramètre à un ensemble de procédures stockées dans la base de données source.</span><span class="sxs-lookup"><span data-stu-id="5a743-171">During the data export step, the cutoff time is passed as a parameter to a set of stored procedures in the source database.</span></span> <span data-ttu-id="5a743-172">Ces procédures stockées demandent au système tous les enregistrements ayant été modifiés ou créés après l’heure de coupure.</span><span class="sxs-lookup"><span data-stu-id="5a743-172">These stored procedures query for any records that were changed or created after the cutoff time.</span></span> <span data-ttu-id="5a743-173">Pour la table de faits Sales, la colonne `LastEditedWhen` est utilisée.</span><span class="sxs-lookup"><span data-stu-id="5a743-173">For the Sales fact table, the `LastEditedWhen` column is used.</span></span> <span data-ttu-id="5a743-174">Dans le cas des données de dimension, les tables temporelles de système par version sont utilisées.</span><span class="sxs-lookup"><span data-stu-id="5a743-174">For the dimension data, system-versioned temporal tables are used.</span></span>

3. <span data-ttu-id="5a743-175">Lorsque la migration des données est terminée, mettez à jour la table qui stocke les heures de coupure.</span><span class="sxs-lookup"><span data-stu-id="5a743-175">When the data migration is complete, update the table that stores the cutoff times.</span></span>

<span data-ttu-id="5a743-176">Il est également utile d’enregistrer un *lignage* pour chaque processus ELT exécuté.</span><span class="sxs-lookup"><span data-stu-id="5a743-176">It's also useful to record a *lineage* for each ELT run.</span></span> <span data-ttu-id="5a743-177">Pour un enregistrement donné, le lignage associe cet enregistrement avec le processus ELT exécuté qui a produit les données.</span><span class="sxs-lookup"><span data-stu-id="5a743-177">For a given record, the lineage associates that record with the ELT run that produced the data.</span></span> <span data-ttu-id="5a743-178">Pour chaque exécution du processus ETL, un enregistrement de lignage est créé pour chaque table, montrant les heures de début et de fin du chargement.</span><span class="sxs-lookup"><span data-stu-id="5a743-178">For each ETL run, a new lineage record is created for every table, showing the starting and ending load times.</span></span> <span data-ttu-id="5a743-179">Les clés de lignage pour chaque enregistrement sont stockées dans les tables de dimension et de faits.</span><span class="sxs-lookup"><span data-stu-id="5a743-179">The lineage keys for each record are stored in the dimension and fact tables.</span></span>

![Capture d’écran de la table de dimension des villes](./images/city-dimension-table.png)

<span data-ttu-id="5a743-181">Lorsqu’un nouveau lot de données est chargé dans l’entrepôt de données, actualisez le modèle Analysis Services tabulaire.</span><span class="sxs-lookup"><span data-stu-id="5a743-181">After a new batch of data is loaded into the warehouse, refresh the Analysis Services tabular model.</span></span> <span data-ttu-id="5a743-182">Voir [Actualisation asynchrone avec l’API REST](/azure/analysis-services/analysis-services-async-refresh).</span><span class="sxs-lookup"><span data-stu-id="5a743-182">See [Asynchronous refresh with the REST API](/azure/analysis-services/analysis-services-async-refresh).</span></span>

## <a name="data-cleansing"></a><span data-ttu-id="5a743-183">Nettoyage des données</span><span class="sxs-lookup"><span data-stu-id="5a743-183">Data cleansing</span></span>

<span data-ttu-id="5a743-184">Le nettoyage des données doit faire partie du processus ELT.</span><span class="sxs-lookup"><span data-stu-id="5a743-184">Data cleansing should be part of the ELT process.</span></span> <span data-ttu-id="5a743-185">Dans cette architecture de référence, la table incluant la population urbaine fournit des données incorrectes, car certaines villes sont associées à une population égale à zéro, sans doute parce qu’aucune donnée n’est disponible.</span><span class="sxs-lookup"><span data-stu-id="5a743-185">In this reference architecture, one source of bad data is the city population table, where some cities have zero population, perhaps because no data was available.</span></span> <span data-ttu-id="5a743-186">Lors du traitement, le pipeline ELT supprime ces villes de la table incluant la population urbaine.</span><span class="sxs-lookup"><span data-stu-id="5a743-186">During processing, the ELT pipeline removes those cities from the city population table.</span></span> <span data-ttu-id="5a743-187">Effectuez le nettoyage des données sur les tables de mise en lots plutôt que sur les tables externes.</span><span class="sxs-lookup"><span data-stu-id="5a743-187">Perform data cleansing on staging tables, rather than external tables.</span></span>

<span data-ttu-id="5a743-188">Voici la procédure stockée qui supprime les villes dont la population est égale à zéro de la table incluant la population urbaine.</span><span class="sxs-lookup"><span data-stu-id="5a743-188">Here is the stored procedure that removes the cities with zero population from the City Population table.</span></span> <span data-ttu-id="5a743-189">(Vous trouverez le fichier source [ici](https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/citypopulation/%5BIntegration%5D.%5BMigrateExternalCityPopulationData%5D.sql).)</span><span class="sxs-lookup"><span data-stu-id="5a743-189">(You can find the source file [here](https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/citypopulation/%5BIntegration%5D.%5BMigrateExternalCityPopulationData%5D.sql).)</span></span>

```sql
DELETE FROM [Integration].[CityPopulation_Staging]
WHERE RowNumber in (SELECT DISTINCT RowNumber
FROM [Integration].[CityPopulation_Staging]
WHERE POPULATION = 0
GROUP BY RowNumber
HAVING COUNT(RowNumber) = 4)
```

## <a name="external-data-sources"></a><span data-ttu-id="5a743-190">Sources de données externes</span><span class="sxs-lookup"><span data-stu-id="5a743-190">External data sources</span></span>

<span data-ttu-id="5a743-191">Souvent, les entrepôts de données consolident les données provenant de plusieurs sources.</span><span class="sxs-lookup"><span data-stu-id="5a743-191">Data warehouses often consolidate data from multiple sources.</span></span> <span data-ttu-id="5a743-192">Cette architecture de référence charge une source de données externe qui contient des données démographiques.</span><span class="sxs-lookup"><span data-stu-id="5a743-192">This reference architecture loads an external data source that contains demographics data.</span></span> <span data-ttu-id="5a743-193">Ce jeu de données est disponible dans le stockage d’objets blob Azure associé à l’exemple [WorldWideImportersDW](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/sample-scripts/polybase).</span><span class="sxs-lookup"><span data-stu-id="5a743-193">This dataset is available in Azure blob storage as part of the [WorldWideImportersDW](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/sample-scripts/polybase) sample.</span></span>

<span data-ttu-id="5a743-194">Azure Data Factory peut copier directement les données depuis le stockage d’objets blob, à l’aide du [connecteur de stockage d’objets blob](/azure/data-factory/connector-azure-blob-storage).</span><span class="sxs-lookup"><span data-stu-id="5a743-194">Azure Data Factory can copy directly from blob storage, using the [blob storage connector](/azure/data-factory/connector-azure-blob-storage).</span></span> <span data-ttu-id="5a743-195">Toutefois, ce connecteur nécessite une chaîne de connexion ou d’une signature d’accès partagé, afin qu’elle ne puisse être utilisée pour copier un objet blob avec un accès en lecture public.</span><span class="sxs-lookup"><span data-stu-id="5a743-195">However, the connector requires a connection string or a shared access signature, so it can't be used to copy a blob with public read access.</span></span> <span data-ttu-id="5a743-196">Pour résoudre ce problème, vous pouvez utiliser PolyBase pour créer une table externe sur le stockage d’objets blob, puis copier les tables externes dans SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="5a743-196">As a workaround, you can use PolyBase to create an external table over Blob storage and then copy the external tables into SQL Data Warehouse.</span></span>

## <a name="handling-large-binary-data"></a><span data-ttu-id="5a743-197">Gestion de données binaires volumineuses</span><span class="sxs-lookup"><span data-stu-id="5a743-197">Handling large binary data</span></span>

<span data-ttu-id="5a743-198">Dans la base de données source, la table Cities inclut une colonne Location qui présente un type de données spatiales [geography](/sql/t-sql/spatial-geography/spatial-types-geography).</span><span class="sxs-lookup"><span data-stu-id="5a743-198">In the source database, the Cities table has a Location column that holds a [geography](/sql/t-sql/spatial-geography/spatial-types-geography) spatial data type.</span></span> <span data-ttu-id="5a743-199">SQL Data Warehouse ne prend pas en charge le type **geography** en mode natif. Ce champ est donc converti en type **varbinary** pendant le chargement.</span><span class="sxs-lookup"><span data-stu-id="5a743-199">SQL Data Warehouse doesn't support the **geography** type natively, so this field is converted to a **varbinary** type during loading.</span></span> <span data-ttu-id="5a743-200">(Voir [Utilisation de solutions de contournement pour les types de données non pris en charge](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types#unsupported-data-types).)</span><span class="sxs-lookup"><span data-stu-id="5a743-200">(See [Workarounds for unsupported data types](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types#unsupported-data-types).)</span></span>

<span data-ttu-id="5a743-201">Toutefois, PolyBase prend en charge une taille de colonne maximale de `varbinary(8000)`, ce qui signifie que certaines données peuvent être tronquées.</span><span class="sxs-lookup"><span data-stu-id="5a743-201">However, PolyBase supports a maximum column size of `varbinary(8000)`, which means some data could be truncated.</span></span> <span data-ttu-id="5a743-202">Pour résoudre ce problème, vous pouvez scinder les données en segments pendant l’exportation, puis les réassembler, comme suit :</span><span class="sxs-lookup"><span data-stu-id="5a743-202">A workaround for this problem is to break the data up into chunks during export, and then reassemble the chunks, as follows:</span></span>

1. <span data-ttu-id="5a743-203">Créez une table de mise en lots temporaire pour la colonne Location.</span><span class="sxs-lookup"><span data-stu-id="5a743-203">Create a temporary staging table for the Location column.</span></span>

2. <span data-ttu-id="5a743-204">Pour chaque ville, fractionnez les données de localisation en segments de 8 000 octets, ce qui entraîne la création de 1 &ndash; N lignes pour chaque ville.</span><span class="sxs-lookup"><span data-stu-id="5a743-204">For each city, split the location data into 8000-byte chunks, resulting in 1 &ndash; N rows for each city.</span></span>

3. <span data-ttu-id="5a743-205">Pour les réassembler, convertissez les lignes en colonnes avec l’opérateur T-SQL [PIVOT](/sql/t-sql/queries/from-using-pivot-and-unpivot), puis concaténez les valeurs des colonnes pour chaque ville.</span><span class="sxs-lookup"><span data-stu-id="5a743-205">To reassemble the chunks, use the T-SQL [PIVOT](/sql/t-sql/queries/from-using-pivot-and-unpivot) operator to convert rows into columns and then concatenate the column values for each city.</span></span>

<span data-ttu-id="5a743-206">La difficulté, c’est que chaque ville peut être scindée en un nombre de lignes spécifique, selon la taille des données géographiques.</span><span class="sxs-lookup"><span data-stu-id="5a743-206">The challenge is that each city will be split into a different number of rows, depending on the size of geography data.</span></span> <span data-ttu-id="5a743-207">Pour que l’opérateur PIVOT fonctionne, chaque ville doit présenter le même nombre de lignes.</span><span class="sxs-lookup"><span data-stu-id="5a743-207">For the PIVOT operator to work, every city must have the same number of rows.</span></span> <span data-ttu-id="5a743-208">À cette fin, la requête T-SQL (que vous pouvez afficher [ici][MergeLocation]) s’efforce de remplir les lignes avec des valeurs vides, afin que chaque ville présente le même nombre de colonnes après l’exécution de l’opérateur PIVOT.</span><span class="sxs-lookup"><span data-stu-id="5a743-208">To make this work, the T-SQL query (which you can view [here][MergeLocation]) does some tricks to pad out the rows with blank values, so that every city has the same number of columns after the pivot.</span></span> <span data-ttu-id="5a743-209">La requête obtenue s’avère beaucoup plus rapide qu’un bouclage dans les lignes, au cas par cas.</span><span class="sxs-lookup"><span data-stu-id="5a743-209">The resulting query turns out to be much faster than looping through the rows one at a time.</span></span>

<span data-ttu-id="5a743-210">La même approche est utilisée pour les données d’image.</span><span class="sxs-lookup"><span data-stu-id="5a743-210">The same approach is used for image data.</span></span>

## <a name="slowly-changing-dimensions"></a><span data-ttu-id="5a743-211">Dimensions à variation lente</span><span class="sxs-lookup"><span data-stu-id="5a743-211">Slowly changing dimensions</span></span>

<span data-ttu-id="5a743-212">Les données de dimension sont relativement statiques, mais elle peuvent changer.</span><span class="sxs-lookup"><span data-stu-id="5a743-212">Dimension data is relatively static, but it can change.</span></span> <span data-ttu-id="5a743-213">Par exemple, un produit peut être réaffecté à une autre catégorie.</span><span class="sxs-lookup"><span data-stu-id="5a743-213">For example, a product might get reassigned to a different product category.</span></span> <span data-ttu-id="5a743-214">Il existe plusieurs approches permettant de gérer les dimensions à variation lente.</span><span class="sxs-lookup"><span data-stu-id="5a743-214">There are several approaches to handling slowly changing dimensions.</span></span> <span data-ttu-id="5a743-215">Une technique courante, appelée [Type 2](https://wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row), consiste à ajouter un nouvel enregistrement chaque fois qu’une dimension est modifiée.</span><span class="sxs-lookup"><span data-stu-id="5a743-215">A common technique, called [Type 2](https://wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row), is to add a new record whenever a dimension changes.</span></span>

<span data-ttu-id="5a743-216">Pour implémenter l’approche Type 2, les tables de dimension requièrent des colonnes supplémentaires, qui spécifient la plage de dates effective d’un enregistrement donné.</span><span class="sxs-lookup"><span data-stu-id="5a743-216">In order to implement the Type 2 approach, dimension tables need additional columns that specify the effective date range for a given record.</span></span> <span data-ttu-id="5a743-217">En outre, les clés primaires de la base de données source sont dupliquées, donc la table de dimension doit avoir une clé primaire artificielle.</span><span class="sxs-lookup"><span data-stu-id="5a743-217">Also, primary keys from the source database will be duplicated, so the dimension table must have an artificial primary key.</span></span>

<span data-ttu-id="5a743-218">L’illustration suivante représente la table Dimension.City.</span><span class="sxs-lookup"><span data-stu-id="5a743-218">The following image shows the Dimension.City table.</span></span> <span data-ttu-id="5a743-219">La colonne `WWI City ID` est la clé primaire provenant de la base de données source.</span><span class="sxs-lookup"><span data-stu-id="5a743-219">The `WWI City ID` column is the primary key from the source database.</span></span> <span data-ttu-id="5a743-220">La colonne `City Key` est une clé artificielle générée lors du pipeline ETL.</span><span class="sxs-lookup"><span data-stu-id="5a743-220">The `City Key` column is an artificial key generated during the ETL pipeline.</span></span> <span data-ttu-id="5a743-221">Notez également que la table inclut des colonnes `Valid From` et `Valid To`, qui définissent la plage lorsque chaque ligne était valide.</span><span class="sxs-lookup"><span data-stu-id="5a743-221">Also notice that the table has `Valid From` and `Valid To` columns, which define the range when each row was valid.</span></span> <span data-ttu-id="5a743-222">Les valeurs actuelles incluent un paramètre `Valid To` égal à « 9999-12-31 ».</span><span class="sxs-lookup"><span data-stu-id="5a743-222">Current values have a `Valid To` equal to '9999-12-31'.</span></span>

![Capture d’écran de la table de dimension des villes](./images/city-dimension-table.png)

<span data-ttu-id="5a743-224">L’avantage de cette approche est qu’elle conserve les données d’historique, qui peuvent être utiles pour l’analyse.</span><span class="sxs-lookup"><span data-stu-id="5a743-224">The advantage of this approach is that it preserves historical data, which can be valuable for analysis.</span></span> <span data-ttu-id="5a743-225">Toutefois, cela signifie également que la même entité sera associée à plusieurs lignes.</span><span class="sxs-lookup"><span data-stu-id="5a743-225">However, it also means there will be multiple rows for the same entity.</span></span> <span data-ttu-id="5a743-226">Par exemple, voici les enregistrements qui correspondent à la valeur `WWI City ID` = 28561 :</span><span class="sxs-lookup"><span data-stu-id="5a743-226">For example, here are the records that match `WWI City ID` = 28561:</span></span>

![Deuxième capture d’écran de la table de dimension des villes](./images/city-dimension-table-2.png)

<span data-ttu-id="5a743-228">Associez chaque fait de la table Sales avec une ligne unique de la table de dimension City correspondant à la date de la facture.</span><span class="sxs-lookup"><span data-stu-id="5a743-228">For each Sales fact, you want to associate that fact with a single row in City dimension table, corresponding to the invoice date.</span></span> <span data-ttu-id="5a743-229">Dans le cadre du processus ETL, créez une colonne supplémentaire.</span><span class="sxs-lookup"><span data-stu-id="5a743-229">As part of the ETL process, create an additional column that</span></span> 

<span data-ttu-id="5a743-230">La requête T-SQL suivante crée une table temporaire qui associe chaque facture à la clé City Key correcte de la table de dimension City.</span><span class="sxs-lookup"><span data-stu-id="5a743-230">The following T-SQL query creates a temporary table that associates each invoice with the correct City Key from the City dimension table.</span></span>

```sql
CREATE TABLE CityHolder
WITH (HEAP , DISTRIBUTION = HASH([WWI Invoice ID]))
AS
SELECT DISTINCT s1.[WWI Invoice ID] AS [WWI Invoice ID],
                c.[City Key] AS [City Key]
    FROM [Integration].[Sale_Staging] s1
    CROSS APPLY (
                SELECT TOP 1 [City Key]
                    FROM [Dimension].[City]
                WHERE [WWI City ID] = s1.[WWI City ID]
                    AND s1.[Last Modified When] > [Valid From]
                    AND s1.[Last Modified When] <= [Valid To]
                ORDER BY [Valid From], [City Key] DESC
                ) c

```

<span data-ttu-id="5a743-231">Cette table est utilisée pour remplir une colonne dans la table de faits Sales :</span><span class="sxs-lookup"><span data-stu-id="5a743-231">This table is used to populate a column in the Sales fact table:</span></span>

```sql
UPDATE [Integration].[Sale_Staging]
SET [Integration].[Sale_Staging].[WWI Customer ID] =  CustomerHolder.[WWI Customer ID]
```

<span data-ttu-id="5a743-232">Cette colonne permet à une requête Power BI de rechercher l’enregistrement City correct pour une facture client donnée.</span><span class="sxs-lookup"><span data-stu-id="5a743-232">This column enables a Power BI query to find the correct City record for a given sales invoice.</span></span>

## <a name="security-considerations"></a><span data-ttu-id="5a743-233">Considérations relatives à la sécurité</span><span class="sxs-lookup"><span data-stu-id="5a743-233">Security considerations</span></span>

<span data-ttu-id="5a743-234">Pour renforcer la sécurité, vous pouvez utiliser des [points de terminaison de service réseau virtuel](/azure/virtual-network/virtual-network-service-endpoints-overview) pour sécuriser les ressources du service Azure à votre réseau virtuel.</span><span class="sxs-lookup"><span data-stu-id="5a743-234">For additional security, you can use [Virtual Network service endpoints](/azure/virtual-network/virtual-network-service-endpoints-overview) to secure Azure service resources to only your virtual network.</span></span> <span data-ttu-id="5a743-235">Cela supprime complètement tout accès Internet public à ces ressources, en autorisant le trafic uniquement à partir de votre réseau virtuel.</span><span class="sxs-lookup"><span data-stu-id="5a743-235">This fully removes public Internet access to those resources, allowing traffic only from your virtual network.</span></span>

<span data-ttu-id="5a743-236">Grâce à cette approche, vous créez un réseau virtuel dans Azure, puis créez des points de terminaison de service privés pour les services Azure.</span><span class="sxs-lookup"><span data-stu-id="5a743-236">With this approach, you create a VNet in Azure and then create private service endpoints for Azure services.</span></span> <span data-ttu-id="5a743-237">Ces services sont ensuite limités au trafic à partir de ce réseau virtuel.</span><span class="sxs-lookup"><span data-stu-id="5a743-237">Those services are then restricted to traffic from that virtual network.</span></span> <span data-ttu-id="5a743-238">Vous pouvez également y accéder à partir de votre réseau local, via une passerelle.</span><span class="sxs-lookup"><span data-stu-id="5a743-238">You can also reach them from your on-premises network through a gateway.</span></span>

<span data-ttu-id="5a743-239">Notez les limitations suivantes :</span><span class="sxs-lookup"><span data-stu-id="5a743-239">Be aware of the following limitations:</span></span>

- <span data-ttu-id="5a743-240">Au moment où cette architecture de référence a été créée, les points de terminaison de service de réseau virtuel étaient pris en charge pour le stockage Azure et Azure SQL Data Warehouse, mais non pour le service Azure Analysis.</span><span class="sxs-lookup"><span data-stu-id="5a743-240">At the time this reference architecture was created, VNet service endpoints are supported for Azure Storage and Azure SQL Data Warehouse, but not for Azure Analysis Service.</span></span> <span data-ttu-id="5a743-241">Vérifiez l’état le plus récent [ici](https://azure.microsoft.com/updates/?product=virtual-network).</span><span class="sxs-lookup"><span data-stu-id="5a743-241">Check the latest status [here](https://azure.microsoft.com/updates/?product=virtual-network).</span></span>

- <span data-ttu-id="5a743-242">Si les points de terminaison de service sont activés pour le stockage Azure, PolyBase ne peut pas copier des données à partir du stockage vers SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="5a743-242">If service endpoints are enabled for Azure Storage, PolyBase cannot copy data from Storage into SQL Data Warehouse.</span></span> <span data-ttu-id="5a743-243">Il existe une atténuation pour ce problème.</span><span class="sxs-lookup"><span data-stu-id="5a743-243">There is a mitigation for this issue.</span></span> <span data-ttu-id="5a743-244">Pour en savoir plus, voir [Impact de l’utilisation des points de terminaison de service de réseau virtuel avec le stockage Azure](/azure/sql-database/sql-database-vnet-service-endpoint-rule-overview?toc=%2fazure%2fvirtual-network%2ftoc.json#impact-of-using-vnet-service-endpoints-with-azure-storage).</span><span class="sxs-lookup"><span data-stu-id="5a743-244">For more information, see [Impact of using VNet Service Endpoints with Azure storage](/azure/sql-database/sql-database-vnet-service-endpoint-rule-overview?toc=%2fazure%2fvirtual-network%2ftoc.json#impact-of-using-vnet-service-endpoints-with-azure-storage).</span></span>

- <span data-ttu-id="5a743-245">Pour déplacer des données locales vers le stockage Azure, vous devrez créer une liste verte regroupant les adresses IP publiques de votre système local ou associées à ExpressRoute.</span><span class="sxs-lookup"><span data-stu-id="5a743-245">To move data from on-premises into Azure Storage, you will need to whitelist public IP addresses from your on-premises or ExpressRoute.</span></span> <span data-ttu-id="5a743-246">Pour en savoir plus, voir [Sécurisation des services Azure pour des réseaux virtuels](/azure/virtual-network/virtual-network-service-endpoints-overview#securing-azure-services-to-virtual-networks).</span><span class="sxs-lookup"><span data-stu-id="5a743-246">For details, see [Securing Azure services to virtual networks](/azure/virtual-network/virtual-network-service-endpoints-overview#securing-azure-services-to-virtual-networks).</span></span>

- <span data-ttu-id="5a743-247">Pour permettre à Analysis Services de lire les données à partir de SQL Data Warehouse, déployez une machine virtuelle Windows sur le réseau virtuel qui contient le point de terminaison de service SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="5a743-247">To enable Analysis Services to read data from SQL Data Warehouse, deploy a Windows VM to the virtual network that contains the SQL Data Warehouse service endpoint.</span></span> <span data-ttu-id="5a743-248">Installez la [passerelle de données locale Azure](/azure/analysis-services/analysis-services-gateway) sur cette machine virtuelle.</span><span class="sxs-lookup"><span data-stu-id="5a743-248">Install [Azure On-premises Data Gateway](/azure/analysis-services/analysis-services-gateway) on this VM.</span></span> <span data-ttu-id="5a743-249">Ensuite, connectez votre service Azure Analysis à la passerelle de données.</span><span class="sxs-lookup"><span data-stu-id="5a743-249">Then connect your Azure Analysis service to the data gateway.</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="5a743-250">Déployer la solution</span><span class="sxs-lookup"><span data-stu-id="5a743-250">Deploy the solution</span></span>

<span data-ttu-id="5a743-251">Pour déployer et exécuter l’implémentation de référence, suivez les étapes du [fichier Readme de GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="5a743-251">To the deploy and run the reference implementation, follow the steps in the [GitHub readme][github].</span></span> <span data-ttu-id="5a743-252">Il déploie les éléments suivants :</span><span class="sxs-lookup"><span data-stu-id="5a743-252">It deploys the following:</span></span>

- <span data-ttu-id="5a743-253">Une machine virtuelle pour simuler un serveur de base de données local.</span><span class="sxs-lookup"><span data-stu-id="5a743-253">A Windows VM to simulate an on-premises database server.</span></span> <span data-ttu-id="5a743-254">Sont inclus SQL Server 2017 et les outils associés, et Power BI Desktop.</span><span class="sxs-lookup"><span data-stu-id="5a743-254">It includes SQL Server 2017 and related tools, along with Power BI Desktop.</span></span>
- <span data-ttu-id="5a743-255">Un compte de stockage Azure qui fournit le stockage d’objets blob pour conserver des données exportées de la base de données SQL Server.</span><span class="sxs-lookup"><span data-stu-id="5a743-255">An Azure storage account that provides Blob storage to hold data exported from the SQL Server database.</span></span>
- <span data-ttu-id="5a743-256">Une instance Azure SQL Data Warehouse.</span><span class="sxs-lookup"><span data-stu-id="5a743-256">An Azure SQL Data Warehouse instance.</span></span>
- <span data-ttu-id="5a743-257">Une instance Azure Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="5a743-257">An Azure Analysis Services instance.</span></span>
- <span data-ttu-id="5a743-258">Azure Data Factory et le pipeline Data Factory associé au travail ELT.</span><span class="sxs-lookup"><span data-stu-id="5a743-258">Azure Data Factory and the Data Factory pipeline for the ELT job.</span></span>

## <a name="related-resources"></a><span data-ttu-id="5a743-259">Ressources associées</span><span class="sxs-lookup"><span data-stu-id="5a743-259">Related resources</span></span>

<span data-ttu-id="5a743-260">Vous pouvez consulter les [exemples de scénarios Azure](/azure/architecture/example-scenario) suivants, qui décrivent des solutions spécifiques utilisant certaines de ces technologies :</span><span class="sxs-lookup"><span data-stu-id="5a743-260">You may want to review the following [Azure example scenarios](/azure/architecture/example-scenario) that demonstrate specific solutions using some of the same technologies:</span></span>

- [<span data-ttu-id="5a743-261">Entreposage et analyse des données pour les ventes et le marketing</span><span class="sxs-lookup"><span data-stu-id="5a743-261">Data warehousing and analytics for sales and marketing</span></span>](/azure/architecture/example-scenario/data/data-warehouse)
- [<span data-ttu-id="5a743-262">ETL hybride avec des instances SSIS locales existantes et Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="5a743-262">Hybrid ETL with existing on-premises SSIS and Azure Data Factory</span></span>](/azure/architecture/example-scenario/data/hybrid-etl-with-adf)

<!-- links -->

[adf]: /azure/data-factory
[github]: https://github.com/mspnp/azure-data-factory-sqldw-elt-pipeline
[MergeLocation]: https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/city/%5BIntegration%5D.%5BMergeLocation%5D.sql
[wwi]: /sql/sample/world-wide-importers/wide-world-importers-oltp-database
