---
title: Scoring par lots des modèles Python sur Azure
description: Générez une solution scalable pour le scoring par lots des modèles selon une planification en parallèle à l’aide d’Azure Batch AI.
author: njray
ms.date: 12/13/18
ms.custom: azcat-ai
ms.openlocfilehash: 93fc0c81663931c0a8b0f54b41934287056e6953
ms.sourcegitcommit: fb22348f917a76e30a6c090fcd4a18decba0b398
ms.translationtype: HT
ms.contentlocale: fr-FR
ms.lasthandoff: 12/16/2018
ms.locfileid: "53450822"
---
# <a name="batch-scoring-of-python-models-on-azure"></a><span data-ttu-id="48d75-103">Scoring par lots des modèles Python sur Azure</span><span class="sxs-lookup"><span data-stu-id="48d75-103">Batch scoring of Python models on Azure</span></span>

<span data-ttu-id="48d75-104">Cette architecture de référence montre comment générer une solution scalable pour le scoring par lots de nombreux modèles selon une planification en parallèle à l’aide d’Azure Batch AI.</span><span class="sxs-lookup"><span data-stu-id="48d75-104">This reference architecture shows how to build a scalable solution for batch scoring many models on a schedule in parallel using Azure Batch AI.</span></span> <span data-ttu-id="48d75-105">La solution peut être utilisée comme modèle et appliquée à différents problèmes.</span><span class="sxs-lookup"><span data-stu-id="48d75-105">The solution can be used as a template and can generalize to different problems.</span></span>

<span data-ttu-id="48d75-106">Une implémentation de référence pour cette architecture est disponible sur [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="48d75-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Scoring par lots des modèles Python sur Azure](./_images/batch-scoring-python.png)

<span data-ttu-id="48d75-108">**Scénario** : Cette solution supervise le fonctionnement d’un grand nombre d’appareils dans un paramètre IoT où chaque appareil envoie des lectures de capteurs en permanence.</span><span class="sxs-lookup"><span data-stu-id="48d75-108">**Scenario**: This solution monitors the operation of a large number of devices in an IoT setting where each device sends sensor readings continuously.</span></span> <span data-ttu-id="48d75-109">Il est supposé que chaque appareil dispose de modèles de détection des anomalies préentraînés qui doivent être utilisés pour prédire si une série de mesures, agrégées sur un intervalle de temps prédéfini, correspondent ou non à une anomalie.</span><span class="sxs-lookup"><span data-stu-id="48d75-109">Each device is assumed to have pre-trained anomaly detection models that need to be used to predict whether a series of measurements, that are aggregated over a predefined time interval, correspond to an anomaly or not.</span></span> <span data-ttu-id="48d75-110">Dans les scénarios réels, il peut s’agir d’un flux de lectures de capteurs qui doivent être filtrées et agrégées avant d’être utilisées dans l’entraînement ou le scoring en temps réel.</span><span class="sxs-lookup"><span data-stu-id="48d75-110">In real-world scenarios, this could be a stream of sensor readings that need to be filtered and aggregated before being used in training or real-time scoring.</span></span> <span data-ttu-id="48d75-111">Par souci de simplicité, la solution utilise le même fichier de données lors de l’exécution des travaux de scoring.</span><span class="sxs-lookup"><span data-stu-id="48d75-111">For simplicity, the solution uses the same data file when executing scoring jobs.</span></span>

## <a name="architecture"></a><span data-ttu-id="48d75-112">Architecture</span><span class="sxs-lookup"><span data-stu-id="48d75-112">Architecture</span></span>

<span data-ttu-id="48d75-113">Cette architecture est constituée des composants suivants :</span><span class="sxs-lookup"><span data-stu-id="48d75-113">This architecture consists of the following components:</span></span>

<span data-ttu-id="48d75-114">[Azure Event Hubs][event-hubs].</span><span class="sxs-lookup"><span data-stu-id="48d75-114">[Azure Event Hubs][event-hubs].</span></span> <span data-ttu-id="48d75-115">Ce service d’ingestion de messages peut recevoir des millions de messages d’événement par seconde.</span><span class="sxs-lookup"><span data-stu-id="48d75-115">This message ingestion service can ingest millions of event messages per second.</span></span> <span data-ttu-id="48d75-116">Dans cette architecture, les capteurs envoient un flux de données au hub d’événements.</span><span class="sxs-lookup"><span data-stu-id="48d75-116">In this architecture, sensors send a stream of data to the event hub.</span></span>

<span data-ttu-id="48d75-117">[Azure Stream Analytics][stream-analytics].</span><span class="sxs-lookup"><span data-stu-id="48d75-117">[Azure Stream Analytics][stream-analytics].</span></span> <span data-ttu-id="48d75-118">Moteur de traitement des événements.</span><span class="sxs-lookup"><span data-stu-id="48d75-118">An event-processing engine.</span></span> <span data-ttu-id="48d75-119">Un travail Stream Analytics lit les flux de données provenant du hub d’événements et effectue le traitement des flux.</span><span class="sxs-lookup"><span data-stu-id="48d75-119">A Stream Analytics job reads the data streams from the event hub and performs stream processing.</span></span>

<span data-ttu-id="48d75-120">[Azure Batch AI][batch-ai].</span><span class="sxs-lookup"><span data-stu-id="48d75-120">[Azure Batch AI][batch-ai].</span></span> <span data-ttu-id="48d75-121">Ce moteur de calcul distribué est utilisé pour entraîner et tester les modèles de machine learning et d’intelligence artificielle à grande échelle dans Azure.</span><span class="sxs-lookup"><span data-stu-id="48d75-121">This distributed computing engine is used to train and test machine learning and AI models at scale in Azure.</span></span> <span data-ttu-id="48d75-122">Batch AI crée des machines virtuelles à la demande avec une option de mise à l’échelle automatique, où chaque nœud du cluster Batch AI exécute un travail de scoring pour un capteur spécifique.</span><span class="sxs-lookup"><span data-stu-id="48d75-122">Batch AI creates virtual machines on demand with an automatic scaling option, where each node in the Batch AI cluster runs a scoring job for a specific sensor.</span></span> <span data-ttu-id="48d75-123">Le [script][python-script] Python de scoring s’exécute dans des conteneurs Docker qui sont créés sur chaque nœud du cluster, où il lit les données de capteurs pertinentes, génère des prédictions et les stocke dans le stockage Blob.</span><span class="sxs-lookup"><span data-stu-id="48d75-123">The scoring Python [script][python-script] runs in Docker containers that are created on each node of the cluster, where it reads the relevant sensor data, generates predictions and stores them in Blob storage.</span></span>

<span data-ttu-id="48d75-124">[Stockage Blob Azure][storage].</span><span class="sxs-lookup"><span data-stu-id="48d75-124">[Azure Blob Storage][storage].</span></span> <span data-ttu-id="48d75-125">Les conteneurs d’objets blob sont utilisés pour stocker les modèles préentraînés, les données et les prédictions de sortie.</span><span class="sxs-lookup"><span data-stu-id="48d75-125">Blob containers are used to store the pretrained models, the data, and the output predictions.</span></span> <span data-ttu-id="48d75-126">Les modèles sont chargés sur le stockage Blob dans le notebook [create\_resources.ipynb][create-resources].</span><span class="sxs-lookup"><span data-stu-id="48d75-126">The models are uploaded to Blob storage in the [create\_resources.ipynb][create-resources] notebook.</span></span> <span data-ttu-id="48d75-127">Ces modèles [SVM à une classe][one-class-svm] sont entraînés sur des données qui représentent les valeurs de différents capteurs pour différents appareils.</span><span class="sxs-lookup"><span data-stu-id="48d75-127">These [one-class SVM][one-class-svm] models are trained on data that represents values of different sensors for different devices.</span></span> <span data-ttu-id="48d75-128">Cette solution part du principe que les valeurs de données sont agrégées sur un intervalle de temps fixe.</span><span class="sxs-lookup"><span data-stu-id="48d75-128">This solution assumes that the data values are aggregated over a fixed interval of time.</span></span>

<span data-ttu-id="48d75-129">[Azure Logic Apps][logic-apps].</span><span class="sxs-lookup"><span data-stu-id="48d75-129">[Azure Logic Apps][logic-apps].</span></span> <span data-ttu-id="48d75-130">Cette solution crée une application logique qui exécute toutes les heures des travaux Batch AI.</span><span class="sxs-lookup"><span data-stu-id="48d75-130">This solution creates a Logic App that runs hourly Batch AI jobs.</span></span> <span data-ttu-id="48d75-131">Logic Apps offre un moyen facile de créer le workflow du runtime et la planification de la solution.</span><span class="sxs-lookup"><span data-stu-id="48d75-131">Logic Apps provides an easy way to create the runtime workflow and scheduling for the solution.</span></span> <span data-ttu-id="48d75-132">Les travaux Batch AI sont envoyés à l’aide d’un [script][script] Python qui s’exécute également dans un conteneur Docker.</span><span class="sxs-lookup"><span data-stu-id="48d75-132">The Batch AI jobs are submitted using a Python [script][script] that also runs in a Docker container.</span></span>

<span data-ttu-id="48d75-133">[Azure Container Registry][acr].</span><span class="sxs-lookup"><span data-stu-id="48d75-133">[Azure Container Registry][acr].</span></span> <span data-ttu-id="48d75-134">Les images Docker sont utilisées à la fois dans Batch AI et Logic Apps, et sont créées dans le notebook [create\_resources.ipynb][create-resources], puis envoyées (push) à Container Registry.</span><span class="sxs-lookup"><span data-stu-id="48d75-134">Docker images are used in both Batch AI and Logic Apps and are created in the [create\_resources.ipynb][create-resources] notebook, then pushed to Container Registry.</span></span> <span data-ttu-id="48d75-135">Cela représente un moyen pratique d’héberger des images et d’instancier des conteneurs via d’autres services Azure, Logic Apps et Batch AI dans cette solution.</span><span class="sxs-lookup"><span data-stu-id="48d75-135">This provides a convenient way to host images and instantiate containers through other Azure services—Logic Apps and Batch AI in this solution.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="48d75-136">Considérations relatives aux performances</span><span class="sxs-lookup"><span data-stu-id="48d75-136">Performance considerations</span></span>

<span data-ttu-id="48d75-137">Pour les modèles Python standard, il est généralement admis que les processeurs sont suffisants pour gérer la charge de travail.</span><span class="sxs-lookup"><span data-stu-id="48d75-137">For standard Python models, it's generally accepted that CPUs are sufficient to handle the workload.</span></span> <span data-ttu-id="48d75-138">Cette architecture utilise des processeurs.</span><span class="sxs-lookup"><span data-stu-id="48d75-138">This architecture uses CPUs.</span></span> <span data-ttu-id="48d75-139">Toutefois, pour les [charges de travail d’apprentissage profond][deep], les GPU sont généralement beaucoup plus performants que les CPU, dans la mesure où un cluster de CPU important est nécessaire pour obtenir des performances comparables.</span><span class="sxs-lookup"><span data-stu-id="48d75-139">However, for [deep learning workloads][deep], GPUs generally outperform CPUs by a considerable amount—a sizeable cluster of CPUs is usually needed to get comparable performance.</span></span>

### <a name="parallelizing-across-vms-vs-cores"></a><span data-ttu-id="48d75-140">Parallélisation dans les machines virtuelles et les cœurs</span><span class="sxs-lookup"><span data-stu-id="48d75-140">Parallelizing across VMs vs cores</span></span>

<span data-ttu-id="48d75-141">Lors de l’exécution des processus de scoring de nombreux modèles en mode Batch, le travail doit être mis en parallèle sur les machines virtuelles.</span><span class="sxs-lookup"><span data-stu-id="48d75-141">When running scoring processes of many models in batch mode, the jobs need to be parallelized across VMs.</span></span> <span data-ttu-id="48d75-142">Deux approches sont possibles :</span><span class="sxs-lookup"><span data-stu-id="48d75-142">Two approaches are possible:</span></span> 

* <span data-ttu-id="48d75-143">Créer un cluster plus grand avec des machines virtuelles de faible coût.</span><span class="sxs-lookup"><span data-stu-id="48d75-143">Create a larger cluster using low-cost VMs.</span></span>

* <span data-ttu-id="48d75-144">Créer un cluster plus petit avec des machines virtuelles hautes performances et plus de cœurs disponibles sur chacune.</span><span class="sxs-lookup"><span data-stu-id="48d75-144">Create a smaller cluster using high performing VMs with more cores available on each.</span></span>

<span data-ttu-id="48d75-145">En général, le scoring des modèles Python standard n’est pas aussi exigeant que celui des modèles d’apprentissage profond, et un petit cluster doit être en mesure de gérer efficacement un grand nombre de modèles en file d’attente.</span><span class="sxs-lookup"><span data-stu-id="48d75-145">In general, scoring of standard Python models is not as demanding as scoring of deep learning models, and a small cluster should be able to handle a large number of queued models efficiently.</span></span> <span data-ttu-id="48d75-146">Vous pouvez accroître le nombre de nœuds de cluster à mesure que les tailles des jeux de données augmentent.</span><span class="sxs-lookup"><span data-stu-id="48d75-146">You can increase the number of cluster nodes as the dataset sizes increase.</span></span>

<span data-ttu-id="48d75-147">Pour des raisons pratiques dans ce scénario, une tâche de scoring est envoyée au sein d’un travail Batch AI unique.</span><span class="sxs-lookup"><span data-stu-id="48d75-147">For convenience in this scenario, one scoring task is submitted within a single Batch AI job.</span></span> <span data-ttu-id="48d75-148">Toutefois, il peut être plus efficace de scorer plusieurs blocs de données au sein du même travail Batch AI.</span><span class="sxs-lookup"><span data-stu-id="48d75-148">However, it can be more efficient to score multiple data chunks within the same Batch AI job.</span></span> <span data-ttu-id="48d75-149">Dans ce cas, écrivez un code personnalisé à lire dans plusieurs jeux de données et exécutez le script de scoring pour ceux-ci lors d’un travail Batch AI unique.</span><span class="sxs-lookup"><span data-stu-id="48d75-149">In those cases, write custom code to read in multiple datasets and execute the scoring script for those during a single Batch AI job execution.</span></span>

### <a name="file-servers"></a><span data-ttu-id="48d75-150">Serveurs de fichiers</span><span class="sxs-lookup"><span data-stu-id="48d75-150">File servers</span></span>

<span data-ttu-id="48d75-151">Quand vous utilisez Batch AI, vous pouvez choisir plusieurs options de stockage, selon le débit nécessaire à votre scénario.</span><span class="sxs-lookup"><span data-stu-id="48d75-151">When using Batch AI, you can choose multiple storage options depending on the throughput needed for your scenario.</span></span> <span data-ttu-id="48d75-152">Pour les charges de travail qui demandent peu de débit, l’utilisation du stockage Blob doit suffire.</span><span class="sxs-lookup"><span data-stu-id="48d75-152">For workloads with low throughput requirements, using blob storage should be enough.</span></span> <span data-ttu-id="48d75-153">Sinon, Batch AI prend aussi en charge un [serveur de fichiers Batch AI][bai-file-server], un système NFS à un seul nœud géré, qui peut être monté automatiquement sur les nœuds du cluster pour fournir un emplacement de stockage accessible de façon centralisée pour les travaux.</span><span class="sxs-lookup"><span data-stu-id="48d75-153">Alternatively, Batch AI also supports a [Batch AI File Server][bai-file-server], a managed, single-node NFS, which can be automatically mounted on cluster nodes to provide a centrally accessible storage location for jobs.</span></span> <span data-ttu-id="48d75-154">Dans la plupart des cas, un seul serveur de fichiers s’avère nécessaire dans un espace de travail, et vous pouvez répartir les données de vos travaux d’entraînement dans différents répertoires.</span><span class="sxs-lookup"><span data-stu-id="48d75-154">For most cases, only one file server is needed in a workspace, and you can separate data for your training jobs into different directories.</span></span>

<span data-ttu-id="48d75-155">Si un système NFS à un seul nœud ne convient pas pour vos charges de travail, Batch AI prend en charge d’autres options de stockage, dont [Azure Files][azure-files] et les solutions personnalisées que sont le système de fichiers Gluster ou Lustre.</span><span class="sxs-lookup"><span data-stu-id="48d75-155">If a single-node NFS isn't appropriate for your workloads, Batch AI supports other storage options, including [Azure Files][azure-files] and custom solutions such as a Gluster or Lustre file system.</span></span>

## <a name="management-considerations"></a><span data-ttu-id="48d75-156">Considérations relatives à la gestion</span><span class="sxs-lookup"><span data-stu-id="48d75-156">Management considerations</span></span>

### <a name="monitoring-batch-ai-jobs"></a><span data-ttu-id="48d75-157">Supervision des tâches Batch AI</span><span class="sxs-lookup"><span data-stu-id="48d75-157">Monitoring Batch AI jobs</span></span>

<span data-ttu-id="48d75-158">Il est important de superviser la progression de l’exécution de travaux, mais cela peut s’avérer ardu sur un cluster de nœuds actifs.</span><span class="sxs-lookup"><span data-stu-id="48d75-158">It's important to monitor the progress of running jobs, but it can be a challenge to monitor across a cluster of active nodes.</span></span> <span data-ttu-id="48d75-159">Pour vous faire une idée de l’état global du cluster, accédez au panneau **Batch AI** du [portail Azure][portal] pour inspecter l’état des nœuds du cluster.</span><span class="sxs-lookup"><span data-stu-id="48d75-159">To get a sense of the overall state of the cluster, go to the **Batch AI** blade of the [Azure Portal][portal] to inspect the state of the nodes in the cluster.</span></span> <span data-ttu-id="48d75-160">Si un nœud est inactif ou si un travail a échoué, les journaux d’erreurs sont enregistrés dans le stockage Blob et sont aussi accessibles dans le panneau **Travaux** du portail.</span><span class="sxs-lookup"><span data-stu-id="48d75-160">If a node is inactive or a job has failed, the error logs are saved to blob storage, and are also accessible in the **Jobs** blade of the portal.</span></span>

<span data-ttu-id="48d75-161">Pour une supervision plus complète, connectez les journaux à [Application Insights][ai] ou exécutez des processus distincts pour demander l’état du cluster Batch AI et de ses travaux.</span><span class="sxs-lookup"><span data-stu-id="48d75-161">For richer monitoring, connect logs to [Application Insights][ai], or run separate processes to poll for the state of the Batch AI cluster and its jobs.</span></span>

### <a name="logging-in-batch-ai"></a><span data-ttu-id="48d75-162">Journalisation dans Batch AI</span><span class="sxs-lookup"><span data-stu-id="48d75-162">Logging in Batch AI</span></span>

<span data-ttu-id="48d75-163">Batch AI journalise tous les stdout/stderr dans le compte de stockage Azure associé.</span><span class="sxs-lookup"><span data-stu-id="48d75-163">Batch AI logs all stdout/stderr to the associated Azure storage account.</span></span> <span data-ttu-id="48d75-164">L’utilisation d’un outil de navigation de stockage comme l’[Explorateur Stockage Azure][explorer] facilite la navigation dans les fichiers journaux.</span><span class="sxs-lookup"><span data-stu-id="48d75-164">For easy navigation of the log files, use a storage navigation tool such as [Azure Storage Explorer][explorer].</span></span>

<span data-ttu-id="48d75-165">Quand vous déployez cette architecture de référence, vous pouvez configurer un système de journalisation plus simple.</span><span class="sxs-lookup"><span data-stu-id="48d75-165">When you deploy this reference architecture, you have the option to set up a simpler logging system.</span></span> <span data-ttu-id="48d75-166">Avec cette option, tous les journaux des différents travaux sont enregistrés dans un même répertoire de votre conteneur d’objets blob, comme illustré ci-dessous.</span><span class="sxs-lookup"><span data-stu-id="48d75-166">With this option, all the logs across the different jobs are saved to the same directory in your blob container as shown below.</span></span> <span data-ttu-id="48d75-167">Ces journaux s’avèrent utiles pour superviser la durée de traitement de chaque travail et de chaque image, et vous pouvez ainsi vous faire une idée plus précise de la façon d’optimiser le processus.</span><span class="sxs-lookup"><span data-stu-id="48d75-167">Use these logs to monitor how long it takes for each job and each image to process, so you have a better sense of how to optimize the process.</span></span>

![Explorateur de stockage Azure](./_images/batch-scoring-python-monitor.png)

## <a name="cost-considerations"></a><span data-ttu-id="48d75-169">Considérations relatives au coût</span><span class="sxs-lookup"><span data-stu-id="48d75-169">Cost considerations</span></span>

<span data-ttu-id="48d75-170">Les composants les plus coûteux utilisés dans cette architecture de référence sont les ressources de calcul.</span><span class="sxs-lookup"><span data-stu-id="48d75-170">The most expensive components used in this reference architecture are the compute resources.</span></span>

<span data-ttu-id="48d75-171">La taille du cluster Batch AI peut être mise à l’échelle à la hausse ou à la baisse en fonction des travaux présents dans la file d’attente.</span><span class="sxs-lookup"><span data-stu-id="48d75-171">The Batch AI cluster size scales up and down depending on the jobs in the queue.</span></span> <span data-ttu-id="48d75-172">Avec Batch AI, vous pouvez activer la [mise à l’échelle automatique][automatic-scaling] de deux façons différentes.</span><span class="sxs-lookup"><span data-stu-id="48d75-172">You can enable [automatic scaling][automatic-scaling] with Batch AI in one of two ways.</span></span> <span data-ttu-id="48d75-173">Vous pouvez le faire par programmation, ce que vous pouvez configurer dans le fichier .env lors des [étapes de déploiement][github], ou vous pouvez changer la formule de mise à l’échelle directement dans le portail une fois le cluster créé.</span><span class="sxs-lookup"><span data-stu-id="48d75-173">You can do so programmatically, which can be configured in the .env file that is part of the [deployment steps][github], or you can change the scale formula directly in the portal after the cluster is created.</span></span>

<span data-ttu-id="48d75-174">Pour les tâches qui ne nécessitent pas un traitement immédiat, configurez la formule de mise à l’échelle automatique de sorte que l’état par défaut (minimum) soit un cluster sans nœud.</span><span class="sxs-lookup"><span data-stu-id="48d75-174">For work that doesn't require immediate processing, configure the automatic scaling formula so the default state (minimum) is a cluster of zero nodes.</span></span> <span data-ttu-id="48d75-175">Avec cette configuration, le cluster démarre sans nœud et ne monte en puissance que s’il détecte des tâches dans la file d’attente.</span><span class="sxs-lookup"><span data-stu-id="48d75-175">With this configuration, the cluster starts with zero nodes and only scales up when it detects jobs in the queue.</span></span> <span data-ttu-id="48d75-176">Si le processus de scoring par lots ne s’enclenche que quelques fois par jour, ce paramètre permet de réaliser des économies significatives.</span><span class="sxs-lookup"><span data-stu-id="48d75-176">If the batch scoring process only happens a few times a day or less, this setting enables significant cost savings.</span></span>

<span data-ttu-id="48d75-177">La mise à l’échelle automatique peut ne pas convenir pour les traitements par lots trop rapprochés les uns des autres.</span><span class="sxs-lookup"><span data-stu-id="48d75-177">Automatic scaling may not be appropriate for batch jobs that happen too close to each other.</span></span> <span data-ttu-id="48d75-178">Le temps nécessaire au lancement et à l’arrêt d’un cluster a aussi un coût. De ce fait, si une charge de travail Batch commence seulement quelques minutes après la fin de la tâche précédente, il peut être plus rentable de laisser le cluster s’exécuter entre les tâches.</span><span class="sxs-lookup"><span data-stu-id="48d75-178">The time that it takes for a cluster to spin up and spin down also incur a cost, so if a batch workload begins only a few minutes after the previous job ends, it might be more cost effective to keep the cluster running between jobs.</span></span> <span data-ttu-id="48d75-179">Cela dépend si les processus de scoring sont planifiés pour s’exécuter très fréquemment (toutes les heures, par exemple) ou moins fréquemment (une fois par mois, par exemple).</span><span class="sxs-lookup"><span data-stu-id="48d75-179">That depends on whether scoring processes are scheduled to run at a high frequency (every hour, for example), or less frequently (once a month, for example).</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="48d75-180">Déployer la solution</span><span class="sxs-lookup"><span data-stu-id="48d75-180">Deploy the solution</span></span>

<span data-ttu-id="48d75-181">L’implémentation de référence de cette architecture est disponible sur [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="48d75-181">The reference implementation of this architecture is available on [GitHub][github].</span></span> <span data-ttu-id="48d75-182">Suivez les étapes de configuration ici afin de générer une solution scalable pour le scoring de nombreux modèles en parallèle à l’aide de Batch AI.</span><span class="sxs-lookup"><span data-stu-id="48d75-182">Follow the setup steps there to build a scalable solution for scoring many models in parallel using Batch AI.</span></span>

[acr]: /azure/container-registry/container-registry-intro
[ai]: /azure/application-insights/app-insights-overview
[automatic-scaling]: /azure/batch/batch-automatic-scaling
[azure-files]: /azure/storage/files/storage-files-introduction
[batch-ai]: /azure/batch-ai/
[bai-file-server]: /azure/batch-ai/resource-concepts#file-server
[create-resources]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/create_resources.ipynb
[deep]: /azure/architecture/reference-architectures/ai/batch-scoring-deep-learning
[event-hubs]: /azure/event-hubs/event-hubs-geo-dr
[explorer]: https://azure.microsoft.com/en-us/features/storage-explorer/
[github]: https://github.com/Azure/BatchAIAnomalyDetection
[logic-apps]: /azure/logic-apps/logic-apps-overview
[one-class-svm]: http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
[portal]: https://portal.azure.com
[python-script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/batchai/predict.py
[script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/sched/submit_jobs.py
[storage]: /azure/storage/blobs/storage-blobs-overview
[stream-analytics]: /azure/stream-analytics/
