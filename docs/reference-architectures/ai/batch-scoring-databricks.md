---
title: Scoring par lots de modèles Spark sur Azure Databricks
description: Créez une solution évolutive pour le scoring par lots d’un modèle de classification Apache Spark basé sur une planification à l’aide d’Azure Databricks.
author: njray
ms.date: 02/07/2019
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai
ms.openlocfilehash: 1b6f10edf098ed8d9fa14c16de113fc765372835
ms.sourcegitcommit: c053e6edb429299a0ad9b327888d596c48859d4a
ms.translationtype: HT
ms.contentlocale: fr-FR
ms.lasthandoff: 03/20/2019
ms.locfileid: "58231416"
---
# <a name="batch-scoring-of-spark-models-on-azure-databricks"></a><span data-ttu-id="83d10-103">Scoring par lots de modèles Spark sur Azure Databricks</span><span class="sxs-lookup"><span data-stu-id="83d10-103">Batch scoring of Spark models on Azure Databricks</span></span>

<span data-ttu-id="83d10-104">Cette architecture de référence montre comment construire une solution évolutive pour le scoring par lots d’un modèle de classification Apache Spark basé sur une planification à l’aide d’Azure Databricks, une plateforme d’analyse basée sur Apache Spark optimisée pour Azure.</span><span class="sxs-lookup"><span data-stu-id="83d10-104">This reference architecture shows how to build a scalable solution for batch scoring an Apache Spark classification model on a schedule using Azure Databricks, an Apache Spark-based analytics platform optimized for Azure.</span></span> <span data-ttu-id="83d10-105">La solution peut servir de modèle généralisé à d’autres scénarios.</span><span class="sxs-lookup"><span data-stu-id="83d10-105">The solution can be used as a template that can be generalized to other scenarios.</span></span>

<span data-ttu-id="83d10-106">Une implémentation de référence pour cette architecture est disponible sur [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="83d10-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Scoring par lots de modèles Spark sur Azure Databricks](./_images/batch-scoring-spark.png)

<span data-ttu-id="83d10-108">**Scénario** : Une entreprise de l’industrie lourde veut minimiser les coûts et les temps d’arrêt associés à des défaillances mécaniques imprévues.</span><span class="sxs-lookup"><span data-stu-id="83d10-108">**Scenario**: A business in an asset-heavy industry wants to minimize the costs and downtime associated with unexpected mechanical failures.</span></span> <span data-ttu-id="83d10-109">En utilisant les données IoT collectées sur les machines, elle peut créer un modèle de maintenance prédictive.</span><span class="sxs-lookup"><span data-stu-id="83d10-109">Using IoT data collected from their machines, they can create a predictive maintenance model.</span></span> <span data-ttu-id="83d10-110">Ce modèle permet à l’entreprise de gérer les composants de manière proactive et de les réparer avant qu’ils ne tombent en panne.</span><span class="sxs-lookup"><span data-stu-id="83d10-110">This model enables the business to maintain components proactively and repair them before they fail.</span></span> <span data-ttu-id="83d10-111">En maximisant l’utilisation des composants mécaniques, elle peut contrôler les coûts et réduire les temps d’arrêt.</span><span class="sxs-lookup"><span data-stu-id="83d10-111">By maximizing mechanical component use, they can control costs and reduce downtime.</span></span>

<span data-ttu-id="83d10-112">Un modèle de maintenance prédictive recueille les données des machines et conserve les exemples historiques de défaillances des composants.</span><span class="sxs-lookup"><span data-stu-id="83d10-112">A predictive maintenance model collects data from the machines and retains historical examples of component failures.</span></span> <span data-ttu-id="83d10-113">Le modèle peut ensuite être utilisé pour superviser l’état actuel des composants et prédire si un composant donné connaîtra une défaillance prochainement.</span><span class="sxs-lookup"><span data-stu-id="83d10-113">The model can then be used to monitor the current state of the components and predict if a given component will fail in the near future.</span></span> <span data-ttu-id="83d10-114">Pour les cas d’usage courants et les approches de modélisation, voir le [Guide Azure AI pour les solutions de maintenance prédictive][ai-guide].</span><span class="sxs-lookup"><span data-stu-id="83d10-114">For common use cases and modeling approaches, see [Azure AI guide for predictive maintenance solutions][ai-guide].</span></span>

<span data-ttu-id="83d10-115">Cette architecture de référence a été conçue pour les charges de travail qui sont déclenchées par la présence de nouvelles données recueillies auprès des machines.</span><span class="sxs-lookup"><span data-stu-id="83d10-115">This reference architecture is designed for workloads that are triggered by the presence of new data from the component machines.</span></span> <span data-ttu-id="83d10-116">Le traitement est constitué des étapes suivantes :</span><span class="sxs-lookup"><span data-stu-id="83d10-116">Processing involves the following steps:</span></span>

1. <span data-ttu-id="83d10-117">Ingérer les données de la base de données externe dans un magasin de données Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="83d10-117">Ingest the data from the external data store onto an Azure Databricks data store.</span></span>

2. <span data-ttu-id="83d10-118">Former un modèle Machine Learning en transformant les données en un ensemble de données d’apprentissage, puis en construisant un modèle MLlib Spark.</span><span class="sxs-lookup"><span data-stu-id="83d10-118">Train a machine learning model by transforming the data into a training data set, then building a Spark MLlib model.</span></span> <span data-ttu-id="83d10-119">MLlib se compose des algorithmes Machine Learning les plus courants et des utilitaires optimisés pour tirer parti des capacités d’évolutivité des données Spark.</span><span class="sxs-lookup"><span data-stu-id="83d10-119">MLlib consists of most common machine learning algorithms and utilities optimized to take advantage of Spark data scalability capabilities.</span></span>

3. <span data-ttu-id="83d10-120">Appliquer le modèle formé pour prédire (classer) les défaillances des composants en transformant les données en un ensemble de données de scoring.</span><span class="sxs-lookup"><span data-stu-id="83d10-120">Apply the trained model to predict (classify) component failures by transforming the data into a scoring data set.</span></span> <span data-ttu-id="83d10-121">Noter les données avec le modèle Spark MLLib.</span><span class="sxs-lookup"><span data-stu-id="83d10-121">Score the data with the Spark MLLib model.</span></span>

4. <span data-ttu-id="83d10-122">Stocker les résultats dans la base de données Databricks en vue de leur exploitation post-traitement.</span><span class="sxs-lookup"><span data-stu-id="83d10-122">Store results on the Databricks data store for post-processing consumption.</span></span>

<span data-ttu-id="83d10-123">Des notebooks sont fournis sur [GitHub][github] pour effectuer chacune de ces tâches.</span><span class="sxs-lookup"><span data-stu-id="83d10-123">Notebooks are provided on [GitHub][github] to perform each of these tasks.</span></span>

## <a name="architecture"></a><span data-ttu-id="83d10-124">Architecture</span><span class="sxs-lookup"><span data-stu-id="83d10-124">Architecture</span></span>

<span data-ttu-id="83d10-125">L’architecture définit un flux de données qui est entièrement contenu dans [Azure Databricks][databricks] basé sur un ensemble de [notebooks ][notebooks] exécutés de manière séquentielle.</span><span class="sxs-lookup"><span data-stu-id="83d10-125">The architecture defines a data flow that is entirely contained within [Azure Databricks][databricks] based on a set of sequentially executed [notebooks][notebooks].</span></span> <span data-ttu-id="83d10-126">Elle se compose des éléments suivants :</span><span class="sxs-lookup"><span data-stu-id="83d10-126">It consists of the following components:</span></span>

<span data-ttu-id="83d10-127">**[Fichiers de données][github]**.</span><span class="sxs-lookup"><span data-stu-id="83d10-127">**[Data files][github]**.</span></span> <span data-ttu-id="83d10-128">L’implémentation de référence utilise un jeu de données simulées contenues dans cinq fichiers de données statiques.</span><span class="sxs-lookup"><span data-stu-id="83d10-128">The reference implementation uses a simulated data set contained in five static data files.</span></span>

<span data-ttu-id="83d10-129">**[Ingestion][notebooks]**.</span><span class="sxs-lookup"><span data-stu-id="83d10-129">**[Ingestion][notebooks]**.</span></span> <span data-ttu-id="83d10-130">Le notebook d’ingestion de données télécharge les fichiers de données d’entrée dans une collection de jeux de données Databricks.</span><span class="sxs-lookup"><span data-stu-id="83d10-130">The data ingestion notebook downloads the input data files into a collection of Databricks data sets.</span></span> <span data-ttu-id="83d10-131">Dans un cas réel, les données des appareils IoT seraient diffusées en continu sur un stockage accessible par Databricks, comme Azure SQL Server ou Azure Blob.</span><span class="sxs-lookup"><span data-stu-id="83d10-131">In a real-world scenario, data from IoT devices would stream onto Databricks-accessible storage such as Azure SQL Server or Azure Blob storage.</span></span> <span data-ttu-id="83d10-132">Databricks supporte plusieurs [sources de données][data-sources].</span><span class="sxs-lookup"><span data-stu-id="83d10-132">Databricks supports multiple [data sources][data-sources].</span></span>

<span data-ttu-id="83d10-133">**Pipeline de formation**.</span><span class="sxs-lookup"><span data-stu-id="83d10-133">**Training pipeline**.</span></span> <span data-ttu-id="83d10-134">Ce notebook exécute le notebook technique de fonctionnalités pour créer un jeu de données d’analyse à partir des données ingérées.</span><span class="sxs-lookup"><span data-stu-id="83d10-134">This notebook executes the feature engineering notebook to create an analysis data set from the ingested data.</span></span> <span data-ttu-id="83d10-135">Il exécute ensuite un notebook de construction de modèle qui forme le modèle de Machine Learning à l’aide de la bibliothèque de Machine Learning évolutive [Apache Spark MLlib][mllib].</span><span class="sxs-lookup"><span data-stu-id="83d10-135">It then executes a model building notebook that trains the machine learning model using the [Apache Spark MLlib][mllib] scalable machine learning library.</span></span>

<span data-ttu-id="83d10-136">**Pipeline de scoring**.</span><span class="sxs-lookup"><span data-stu-id="83d10-136">**Scoring pipeline**.</span></span> <span data-ttu-id="83d10-137">Ce notebook exécute le notebook technique de fonctionnalités pour créer un jeu de données de scoring, puis exécute le notebook de scoring.</span><span class="sxs-lookup"><span data-stu-id="83d10-137">This notebook executes the feature engineering notebook to create scoring data set from the ingested data and executes the scoring notebook.</span></span> <span data-ttu-id="83d10-138">Le notebook s’appuie sur le modèle [Spark MLlib][mllib-spark] formé pour produire des prédictions pour les observations dans le jeu de données de scoring.</span><span class="sxs-lookup"><span data-stu-id="83d10-138">The scoring notebook uses the trained [Spark MLlib][mllib-spark] model to generate predictions for the observations in the scoring data set.</span></span> <span data-ttu-id="83d10-139">Les prédictions sont stockées dans le magasin de données des résultats, nouveau jeu de données du magasin de données Databricks.</span><span class="sxs-lookup"><span data-stu-id="83d10-139">The predictions are stored in the results store, a new data set on the Databricks data store.</span></span>

<span data-ttu-id="83d10-140">**Planificateur**.</span><span class="sxs-lookup"><span data-stu-id="83d10-140">**Scheduler**.</span></span> <span data-ttu-id="83d10-141">Un [travail][job] Databricks planifié gère le scoring par lots avec le modèle Spark.</span><span class="sxs-lookup"><span data-stu-id="83d10-141">A scheduled Databricks [job][job] handles batch scoring with the Spark model.</span></span> <span data-ttu-id="83d10-142">Le travail exécute le notebook du pipeline de scoring en transmettant des arguments variables par le biais des paramètres du notebook, ce qui spécifie les détails de construction du jeu de données de scoring et l’emplacement de stockage du jeu de données de résultats.</span><span class="sxs-lookup"><span data-stu-id="83d10-142">The job executes the scoring pipeline notebook, passing variable arguments through notebook parameters to specify the details for constructing the scoring data set and where to store the results data set.</span></span>

<span data-ttu-id="83d10-143">Le scénario est construit comme un flux de pipeline.</span><span class="sxs-lookup"><span data-stu-id="83d10-143">The scenario is constructed as a pipeline flow.</span></span> <span data-ttu-id="83d10-144">Chaque notebook est optimisé pour fonctionner en mode par lots pour chacune des opérations : ingestion, ingénierie des fonctionnalités, construction de modèles et scorings de modèles.</span><span class="sxs-lookup"><span data-stu-id="83d10-144">Each notebook is optimized to perform in a batch setting for each of the operations: ingestion, feature engineering, model building, and model scorings.</span></span> <span data-ttu-id="83d10-145">Pour ce faire, le notebook d’ingénierie des fonctionnalités est conçu pour générer un jeu de données général pour toutes les opérations de formation, d’étalonnage, de test ou de scoring.</span><span class="sxs-lookup"><span data-stu-id="83d10-145">To accomplish this, the feature engineering notebook is designed to generate a general data set for any of the training, calibration, testing, or scoring operations.</span></span> <span data-ttu-id="83d10-146">Dans ce scénario, nous utilisons une stratégie de fractionnement temporel pour ces opérations de sorte que les paramètres du notebook sont utilisés pour définir le filtrage de la plage de dates.</span><span class="sxs-lookup"><span data-stu-id="83d10-146">In this scenario, we use a temporal split strategy for these operations, so the notebook parameters are used to set date-range filtering.</span></span>

<span data-ttu-id="83d10-147">Du fait que le scénario crée un pipeline par lots, nous fournissons un ensemble de notebooks d’examen en option pour explorer la sotie des notebooks de pipeline.</span><span class="sxs-lookup"><span data-stu-id="83d10-147">Because the scenario creates a batch pipeline, we provide a set of optional examination notebooks to explore the output of the pipeline notebooks.</span></span> <span data-ttu-id="83d10-148">Ils sont disponibles dans le dépôt GitHub :</span><span class="sxs-lookup"><span data-stu-id="83d10-148">You can find these in the GitHub repository:</span></span>

- `1a_raw-data_exploring`
- `2a_feature_exploration`
- `2b_model_testing`
- `3b_model_scoring_evaluation`

## <a name="recommendations"></a><span data-ttu-id="83d10-149">Recommandations</span><span class="sxs-lookup"><span data-stu-id="83d10-149">Recommendations</span></span>

<span data-ttu-id="83d10-150">Databricks est configuré pour que vous puissiez charger et déployer vos modèles formés afin de faire des prédictions avec de nouvelles données.</span><span class="sxs-lookup"><span data-stu-id="83d10-150">Databricks is set up so you can load and deploy your trained models to make predictions with new data.</span></span> <span data-ttu-id="83d10-151">Nous avons utilisé Databricks pour ce scénario parce qu’il offre ces avantages supplémentaires :</span><span class="sxs-lookup"><span data-stu-id="83d10-151">We used Databricks for this scenario because it provides these additional advantages:</span></span>

- <span data-ttu-id="83d10-152">Prise en charge de l’authentification unique à l’aide des informations d’identification Azure Active Directory.</span><span class="sxs-lookup"><span data-stu-id="83d10-152">Single sign-on support using Azure Active Directory credentials.</span></span>
- <span data-ttu-id="83d10-153">Planificateur de tâches permettant l’exécution des travaux pour les pipelines de production.</span><span class="sxs-lookup"><span data-stu-id="83d10-153">Job scheduler to execute jobs for production pipelines.</span></span>
- <span data-ttu-id="83d10-154">Notebook entièrement interactif avec collaboration, tableaux de bord, API REST.</span><span class="sxs-lookup"><span data-stu-id="83d10-154">Fully interactive notebook with collaboration, dashboards, REST APIs.</span></span>
- <span data-ttu-id="83d10-155">Clusters illimités pouvant s’adapter à n’importe quelle taille.</span><span class="sxs-lookup"><span data-stu-id="83d10-155">Unlimited clusters that can scale to any size.</span></span>
- <span data-ttu-id="83d10-156">Sécurité avancée, contrôles d’accès en fonction du rôles et journaux d’audit.</span><span class="sxs-lookup"><span data-stu-id="83d10-156">Advanced security, role-based access controls, and audit logs.</span></span>

<span data-ttu-id="83d10-157">Pour interagir avec le service Azure Databricks, utilisez l’interface Databricks [Workspace][workspace] dans un navigateur web ou l’[interface de ligne de commande][cli] (CLI).</span><span class="sxs-lookup"><span data-stu-id="83d10-157">To interact with the Azure Databricks service, use the Databricks [Workspace][workspace] interface in a web browser or the [command-line interface][cli] (CLI).</span></span> <span data-ttu-id="83d10-158">Accédez à l’interface de ligne de commande Databricks depuis n’importe quelle plate-forme prenant en charge Python 2.7.9 à 3.6.</span><span class="sxs-lookup"><span data-stu-id="83d10-158">Access the Databricks CLI from any platform that supports Python 2.7.9 to 3.6.</span></span>

<span data-ttu-id="83d10-159">L’implémentation de référence utilise des [notebooks][notebooks] pour exécuter les tâches séquentiellement.</span><span class="sxs-lookup"><span data-stu-id="83d10-159">The reference implementation uses [notebooks][notebooks] to execute tasks in sequence.</span></span> <span data-ttu-id="83d10-160">Chaque notebook stocke les artefacts de données intermédiaires (formation, test, scoring ou jeux de données de résultats) dans la même base de données que les données d’entrée.</span><span class="sxs-lookup"><span data-stu-id="83d10-160">Each notebook stores intermediate data artifacts (training, test, scoring, or results data sets) to the same data store as the input data.</span></span> <span data-ttu-id="83d10-161">L’objectif est de vous permettre de l’utiliser facilement en fonction de votre cas d’usage particulier.</span><span class="sxs-lookup"><span data-stu-id="83d10-161">The goal is to make it easy for you to use it as needed in your particular use case.</span></span> <span data-ttu-id="83d10-162">En pratique, vous connecteriez votre source de données à votre instance Azure Databricks pour que les notebooks puissent lire et écrire directement dans votre stockage.</span><span class="sxs-lookup"><span data-stu-id="83d10-162">In practice, you would connect your data source to your Azure Databricks instance for the notebooks to read and write directly back into your storage.</span></span>

<span data-ttu-id="83d10-163">Vous pouvez superviser l’exécution du travail via l’interface utilisateur de Databricks, le magasin de données ou l’[interface de ligne de commande][cli] Databricks, si nécessaire.</span><span class="sxs-lookup"><span data-stu-id="83d10-163">You can monitor job execution through the Databricks user interface, the data store, or the Databricks [CLI][cli] as necessary.</span></span> <span data-ttu-id="83d10-164">Supervisez le cluster à l’aide du [journal des événements][log] et des autres [métriques][metrics] que Databricks fournit.</span><span class="sxs-lookup"><span data-stu-id="83d10-164">Monitor the cluster using the [event log][log] and other [metrics][metrics] that Databricks provides.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="83d10-165">Considérations relatives aux performances</span><span class="sxs-lookup"><span data-stu-id="83d10-165">Performance considerations</span></span>

<span data-ttu-id="83d10-166">Un cluster Azure Databricks permet la mise à l’échelle automatique par défaut de sorte que, pendant l’exécution, Databricks réaffecte dynamiquement les Workers pour tenir compte des caractéristiques de votre travail.</span><span class="sxs-lookup"><span data-stu-id="83d10-166">An Azure Databricks cluster enables autoscaling by default so that during runtime, Databricks dynamically reallocates workers to account for the characteristics of your job.</span></span> <span data-ttu-id="83d10-167">Certains éléments de votre pipeline peuvent être plus exigeants que d’autres sur le plan informatique.</span><span class="sxs-lookup"><span data-stu-id="83d10-167">Certain parts of your pipeline may be more computationally demanding than others.</span></span> <span data-ttu-id="83d10-168">Databricks ajoute des Workers supplémentaires pendant ces phases de votre travail (et les supprime lorsqu’ils ne sont plus nécessaires).</span><span class="sxs-lookup"><span data-stu-id="83d10-168">Databricks adds additional workers during these phases of your job (and removes them when they’re no longer needed).</span></span> <span data-ttu-id="83d10-169">La mise à l’échelle automatique facilite l’optimisation de l’[utilisation ducluster][cluster], car vous n’avez pas besoin de dimensionner le cluster en fonction d’une charge de travail.</span><span class="sxs-lookup"><span data-stu-id="83d10-169">Autoscaling makes it easier to achieve high [cluster utilization][cluster], because you don’t need to provision the cluster to match a workload.</span></span>

<span data-ttu-id="83d10-170">De plus, il est possible de développer des pipelines planifiés plus complexes en utilisant [Azure Data Factory][adf] avec Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="83d10-170">Additionally, more complex scheduled pipelines can be developed by using [Azure Data Factory][adf] with Azure Databricks.</span></span>

## <a name="storage-considerations"></a><span data-ttu-id="83d10-171">Considérations relatives au stockage</span><span class="sxs-lookup"><span data-stu-id="83d10-171">Storage considerations</span></span>

<span data-ttu-id="83d10-172">Dans cette implémentation de référence, les données sont stockées directement dans le stockage Databricks pour plus de simplicité.</span><span class="sxs-lookup"><span data-stu-id="83d10-172">In this reference implementation, the data is stored directly within Databricks storage for simplicity.</span></span> <span data-ttu-id="83d10-173">Dans un environnement de production, cependant, les données peuvent être stockées dans un stockage de données cloud tel que [Stockage Blob Azure][blob].</span><span class="sxs-lookup"><span data-stu-id="83d10-173">In a production setting, however, the data can be stored on cloud data storage such as [Azure Blob Storage][blob].</span></span> <span data-ttu-id="83d10-174">[Databricks][databricks-connect] prend également en charge Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka et Hadoop.</span><span class="sxs-lookup"><span data-stu-id="83d10-174">[Databricks][databricks-connect] also supports Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka, and Hadoop.</span></span>

## <a name="cost-considerations"></a><span data-ttu-id="83d10-175">Considérations relatives au coût</span><span class="sxs-lookup"><span data-stu-id="83d10-175">Cost considerations</span></span>

<span data-ttu-id="83d10-176">Azure Databricks est une offre Spark premium avec un coût associé.</span><span class="sxs-lookup"><span data-stu-id="83d10-176">Azure Databricks is a premium Spark offering with an associated cost.</span></span> <span data-ttu-id="83d10-177">En outre, il existe des [niveaux de tarification][pricing] standard et premium pour Databricks.</span><span class="sxs-lookup"><span data-stu-id="83d10-177">In addition, there are standard and premium Databricks [pricing tiers][pricing].</span></span>

<span data-ttu-id="83d10-178">Pour ce scénario, le niveau de tarification standard est suffisant.</span><span class="sxs-lookup"><span data-stu-id="83d10-178">For this scenario, the standard pricing tier is sufficient.</span></span> <span data-ttu-id="83d10-179">Cependant, si votre application spécifique nécessite la mise à l’échelle automatique des clusters pour traiter des charges de travail plus importantes ou des tableaux de bord interactifs de Databricks, le niveau premium pourrait augmenter encore les coûts.</span><span class="sxs-lookup"><span data-stu-id="83d10-179">However, if your specific application requires automatically scaling clusters to handle larger workloads or interactive Databricks dashboards, the premium level could increase costs further.</span></span>

<span data-ttu-id="83d10-180">Les notebooks de la solution peuvent fonctionner sur n’importe quelle plateforme Spark avec un minimum de modifications pour supprimer les packages spécifiques à Databricks.</span><span class="sxs-lookup"><span data-stu-id="83d10-180">The solution notebooks can run on any Spark-based platform with minimal edits to remove the Databricks-specific packages.</span></span> <span data-ttu-id="83d10-181">Envisagez d’utiliser les solutions similaires suivantes pour les différentes plateformes Azure :</span><span class="sxs-lookup"><span data-stu-id="83d10-181">See the following similar solutions for various Azure platforms:</span></span>

- <span data-ttu-id="83d10-182">[Python sur Azure Machine Learning Studio][python-aml]</span><span class="sxs-lookup"><span data-stu-id="83d10-182">[Python on Azure Machine Learning Studio][python-aml]</span></span>
- <span data-ttu-id="83d10-183">[Services SQL Server R][sql-r]</span><span class="sxs-lookup"><span data-stu-id="83d10-183">[SQL Server R services][sql-r]</span></span>
- <span data-ttu-id="83d10-184">[PySpark sur une machine virtuelle Azure Data Science Virtual Machine][py-dvsm]</span><span class="sxs-lookup"><span data-stu-id="83d10-184">[PySpark on an Azure Data Science Virtual Machine][py-dvsm]</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="83d10-185">Déployer la solution</span><span class="sxs-lookup"><span data-stu-id="83d10-185">Deploy the solution</span></span>

<span data-ttu-id="83d10-186">Pour déployer cette architecture de référence, suivez les étapes décrites dans le dépôt  [GitHub][github] afin de construire une solution évolutive de scoring de modèles Spark par lots sur Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="83d10-186">To deploy this reference architecture, follow the steps described in the [GitHub][github] repository to build a scalable solution for scoring Spark models in batch on Azure Databricks.</span></span>

## <a name="related-architectures"></a><span data-ttu-id="83d10-187">Architectures connexes</span><span class="sxs-lookup"><span data-stu-id="83d10-187">Related architectures</span></span>

<span data-ttu-id="83d10-188">Nous avons également élaboré une architecture de référence qui utilise Spark pour construire des [systèmes de recommandation en temps réel][recommendation] avec des scores pré-calculés et hors ligne.</span><span class="sxs-lookup"><span data-stu-id="83d10-188">We have also built a reference architecture that uses Spark for building [real-time recommendation systems][recommendation] with offline, pre-computed scores.</span></span> <span data-ttu-id="83d10-189">Ces systèmes de recommandation font partie de scénarios courants dans lesquels les scores traités par lots.</span><span class="sxs-lookup"><span data-stu-id="83d10-189">These recommendation systems are common scenarios where scores are batch-processed.</span></span>

[adf]: https://azure.microsoft.com/blog/operationalize-azure-databricks-notebooks-using-data-factory/
[ai-guide]: /azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance
[blob]: https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[cli]: https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html
[cluster]: https://docs.azuredatabricks.net/user-guide/clusters/sizing.html
[databricks]: /azure/azure-databricks/
[databricks-connect]: /azure/azure-databricks/databricks-connect-to-data-sources
[data-sources]: https://docs.databricks.com/spark/latest/data-sources/index.html
[github]: https://github.com/Azure/BatchSparkScoringPredictiveMaintenance
[job]: https://docs.databricks.com/user-guide/jobs.html
[log]: https://docs.databricks.com/user-guide/clusters/event-log.html
[metrics]: https://docs.databricks.com/user-guide/clusters/metrics.html
[mllib]: https://docs.databricks.com/spark/latest/mllib/index.html
[mllib-spark]: https://docs.databricks.com/spark/latest/mllib/index.html#apache-spark-mllib
[notebooks]: https://docs.databricks.com/user-guide/notebooks/index.html
[pricing]: https://azure.microsoft.com/en-us/pricing/details/databricks/
[python-aml]: https://gallery.azure.ai/Notebook/Predictive-Maintenance-Modelling-Guide-Python-Notebook-1
[py-dvsm]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-using-PySpark
[recommendation]: /azure/architecture/reference-architectures/ai/real-time-recommendation
[sql-r]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-Modeling-Guide-using-SQL-R-Services-1
[workspace]: https://docs.databricks.com/user-guide/workspace.html
